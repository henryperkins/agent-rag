# Unified Orchestrator & Context Pipeline Design

## Background & Goals
- `/chat` and `/chat/stream` both delegate to the unified orchestrator via `handleEnhancedChat` and `handleChatStream`. Streaming requests now flow through `runSession` with identical planning, retrieval, critique loops, and telemetry hooks, with `createSessionRecorder` cloning every orchestrator event into telemetry while forwarding them to the SSE caller.
- The application uses direct Azure AI Search integration (`backend/src/azure/directSearch.ts`) with hybrid semantic search combining vector similarity, keyword matching (BM25), and L2 semantic reranking via Azure OpenAI Models API.
- Planner outcomes (`backend/src/orchestrator/plan.ts`) use Azure OpenAI structured outputs with JSON schema validation to decide retrieval strategy, following an intent classification pre-step that sets default models and retriever strategies per turn.
- Production-quality agentic RAG requires a single orchestrator responsible for planning, context budgeting, multimodal retrieval, synthesis, critique, and telemetry.

**Goals**
1. Provide a unified orchestration service that powers both synchronous and streaming experiences without duplicating logic.
2. Establish a context pipeline that summarizes history, persists salient memory, and selects the minimal window for each tool call.
3. Enforce planner decisions with tool routing, retrieval fallback ordering, and consistent critic/evaluator loops.
4. Capture structured telemetry (prompts, token budgets, tool usage) for observability and evaluation.

**Non-Goals**
- Building abstraction layers over Azure OpenAI Models API or Azure AI Search REST API.
- Introducing new UI surfaces beyond the telemetry required to visualize pipeline stages.

## Proposed Architecture

### Unified Orchestrator Module
- Maintain `backend/src/orchestrator/index.ts` that exposes `runSession(options)` returning a rich session trace:
  - Inputs: full `AgentMessage[]`, execution mode (`sync` | `stream`), session id, feature flags.
  - Outputs: final answer, citations, activity timeline, emitted events, telemetry bundle.
- Responsibilities:
 1. **Intent Routing** – Classify the active turn (plus recent dialogue) into FAQ, factual lookup, research, or conversational intents using Azure OpenAI structured outputs. Emit `route` telemetry with model + retriever defaults that downstream steps can override.
 2. **Planning** – Invoke `getPlan` once per turn with preprocessed history (see Context Pipeline) and augment response with guardrails (confidence thresholds, fallback heuristics). Uses Azure OpenAI structured outputs via the Responses API (`/responses`) with structured JSON outputs.
 3. **Tool Dispatch** – Route to `retrieveTool`, `webSearchTool`, or a combined branch. Multi-level fallback strategy: hybrid semantic search (high threshold) → hybrid semantic search (lower threshold) → pure vector search. When lazy retrieval is enabled, dispatch returns summary-only references first, deferring full-document loads until the critic requests more evidence. Planner fallbacks must surface rationale when overridden.
 4. **Synthesis** – Call `answerTool` with structured context generated by the context pipeline and attach source metadata. Streams responses using Azure OpenAI Responses API (`/responses` streaming) while honoring the routed model and token cap.
 5. **Critique Loop** – Run `evaluateAnswer` (and future evaluators) using Azure OpenAI structured outputs with JSON schema validation. Manual retry loop with configurable `CRITIC_MAX_RETRIES`. Critic feedback can trigger lazy retrieval to hydrate full documents before regenerating.
 6. **Telemetry** – Emit step-level events (start/finish, tokens in/out, cost estimates) appended to a structured trace using OpenTelemetry, including route decisions, retrieval modes, and lazy summary token counts.
- Existing services (`handleEnhancedChat`, `handleChatStream`) already delegate to the orchestrator; future updates should extend this flow rather than rebuilding parallel pipelines.
- Provide dependency injection for tools and telemetry sinks to make local testing easier (mock retrieval, stub critics).

### Context Pipeline
- **Sanitized History View** – Start from `sanitizeInput` output (`backend/src/middleware/sanitize.ts:6`). Apply a configurable turn limit (e.g., 12 most recent user/assistant messages) and strip redundant assistant echoes.
- **Rolling Summary** – Introduce `conversationSummarizer` module that compacts older turns once the token estimate exceeds threshold T. Persist summary entries in a scratchpad store (in-memory map initially, later external KV).
- **Salience Store** – Capture key facts, user preferences, and unresolved questions using prompt-based extraction after each assistant reply. Store as structured records with metadata (topic, lastSeen turn, decay score).
- **Selection/Compression** – Before calling retrieval or planner, assemble context by:
  1. Latest N raw turns.
  2. Most relevant summary snippets (semantic similarity against current user turn).
  3. Top-K salience notes filtered by freshness.
- **Token Budgeting** – Implement a `ContextBudget` helper that tracks estimated tokens using model-specific tokenizers and trims inputs to stay within configurable ceilings (e.g., 3k for planner, 6k for retrieval). Expose metrics in telemetry.

### Execution Flow
1. **Input Receipt** – `/chat` and `/chat/stream` both call `runSession` with the sanitized message list and a generated session id.
2. **Intent Classification** – Router determines the intent, emits a `route` event, and seeds default model/retriever strategy before planning.
3. **Context Preparation** – Orchestrator builds context snapshot (raw turns + summaries + salience) via Context Pipeline, capturing summary-selection statistics so downstream tooling and telemetry can reason about which bullets were retained or discarded. Uses semantic similarity (via Azure OpenAI `/embeddings` endpoint) to select relevant summary bullets.
4. **Planning** – `getPlan` executes on compacted history using Azure OpenAI Responses API (`/responses`) with structured JSON schema. If confidence < threshold, orchestrator may escalate to dual retrieval (Azure AI Search + Google web search) and annotate reason.
5. **Retrieval** – Execute `retrieveTool` or `lazyRetrieveTool` with `withRetry` resilience wrapper. Primary: hybrid semantic search (vector + BM25 + L2 reranker) via Azure AI Search REST API. Lazy mode returns summary-only references plus callbacks for just-in-time full loads. Fallback 1: Lower reranker threshold. Fallback 2: Pure vector search. All failures logged with decision metadata.
6. **Synthesis & Critique** – `answerTool` consumes curated context via Azure OpenAI Responses API (`/responses`); critic loop runs using structured outputs until acceptance or retry limit. Critic feedback can request full-document hydration when summaries lack coverage. Results appended to trace.
6. **Event Emission** – `createSessionRecorder` subscribes to orchestrator events, persisting sanitized telemetry and mirroring each event to the streaming handler so SSE clients receive the same plan/retrieval/token/critique updates. Sync mode aggregates and returns the final payload + trace.

Note: The orchestrator emits a `tokens` event internally for partial answer chunks; the streaming layer maps this to SSE event `token`. Frontend clients should subscribe to `token`.

7. **Telemetry Persist** – At completion, orchestrator writes trace to telemetry sink (initially in-memory with `/admin/telemetry`, later pluggable via OpenTelemetry exporters).

### Data & API Changes
- **Session Trace Schema** – Define new type in `shared/types.ts` (e.g., `SessionTrace`) capturing steps, messages, tool calls, token usage, errors. Trace entries now include routing metadata, retrieval modes, and lazy summary token metrics.
- **Route & Retrieval Metadata** – `/chat` responses include `traceId`, `plan`, `contextBudget`, `summarySelection`, `criticSummary`, routed model details (`metadata.route`), and retrieval mode (`metadata.retrieval_mode`) inside `metadata`. SSE stream emits `route`, `plan`, `context`, `tool`, and `telemetry` updates, with the telemetry payload embedding summary selection and lazy retrieval statistics for richer UX.
- **Session Identity** – Session ids are either supplied by the client or derived from a stable fingerprint (first non-system turns + client fingerprint) so memory, salience, and telemetry stores remain consistent across turns without leaking between users.
- **Configuration** – Add context budget limits, summary thresholds, and telemetry sinks to `config/app.ts` with sane defaults.

## Implementation Plan

### Phase 1 – Orchestrator Skeleton (1 sprint) ✅ COMPLETED
1. ✅ Created orchestrator module with interfaces and integrated into `/chat` while preserving current outputs.
2. ✅ Updated `/chat/stream` to consume orchestrator events with full SSE streaming support.
3. ✅ Added dependency injection for tools to enable testing with mocked tools.

### Phase 2 – Context Pipeline MVP (1–2 sprints) ✅ COMPLETED
1. ✅ Implemented rolling summaries and salience store with in-memory persistence (`backend/src/orchestrator/memoryStore.ts`).
2. ✅ Introduced context budgeting helper using tiktoken for model-specific token estimation (`backend/src/orchestrator/contextBudget.ts`).
3. ✅ Expanded telemetry to record context components and token costs; exposed via `/admin/telemetry`.

### Phase 3 – Tool Routing & Critique Enforcement (1 sprint) ✅ COMPLETED
1. ✅ Wired planner actions to actual tool invocations (Google Custom Search API, combined retrieval).
2. ✅ Critic loop runs for both sync and streaming flows with consistent metadata.
3. ✅ Frontend displays new events (planning status, context snapshots, critique summary timeline).

### Phase 4 – Hardening (ongoing)
1. Add integration tests covering orchestration permutations (answer, retrieve, web search, fallback vector).
2. Stress-test context budgets with long conversations; refine summary heuristics.
3. Evaluate telemetry storage options (OpenTelemetry exporter, durable store).

## Open Questions
- What persistence layer do we adopt for summaries/salience (Redis, Azure Table, Azure Cosmos DB) once in-memory proves insufficient?
- How do we redact sensitive user data inside stored traces without losing debugging fidelity? (Partially addressed with session telemetry sanitization)
- Should planner confidence thresholds be static or dynamically tuned based on evaluation feedback?
- How can we leverage Azure AI Foundry Evals API (preview) from v1preview.json to systematically evaluate planner and critic performance?
- Should we migrate to Azure OpenAI Responses API streaming format for better token-by-token control?
