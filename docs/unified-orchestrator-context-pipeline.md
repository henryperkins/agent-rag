# Unified Orchestrator & Context Pipeline Design

## Background & Goals
- `/chat` and `/chat/stream` currently execute divergent pipelines: the former uses `handleEnhancedChat` (`backend/src/services/enhancedChatService.ts:8`), while the latter streams tokens directly from Azure OpenAI without planning or critique loops (`backend/src/services/chatStreamService.ts:10`).
- All turn history is forwarded verbatim into retrieval (`backend/src/azure/agenticRetrieval.ts:41`) resulting in uncontrollable context growth, duplicated evidence, and token waste.
- Planner outcomes (`backend/src/agents/advancedPlanner.ts:21`) are not enforced—`action: "web_search"` never triggers `webSearchTool` and `answer` falls back to an LLM call with empty context (`backend/src/services/enhancedChatService.ts:13`).
- Production-quality agentic RAG requires a single orchestrator responsible for planning, context budgeting, multimodal retrieval, synthesis, critique, and telemetry.

**Goals**
1. Provide a unified orchestration service that powers both synchronous and streaming experiences without duplicating logic.
2. Establish a context pipeline that summarizes history, persists salient memory, and selects the minimal window for each tool call.
3. Enforce planner decisions with tool routing, retrieval fallback ordering, and consistent critic/evaluator loops.
4. Capture structured telemetry (prompts, token budgets, tool usage) for observability and evaluation.

**Non-Goals**
- Replacing Azure AI Search Knowledge Agent or vector fallback integrations.
- Introducing new UI surfaces beyond the telemetry required to visualize pipeline stages.

## Proposed Architecture

### Unified Orchestrator Module
- Create `backend/src/orchestrator/index.ts` that exposes `runSession(options)` returning a rich session trace:
  - Inputs: full `AgentMessage[]`, execution mode (`sync` | `stream`), session id, feature flags.
  - Outputs: final answer, citations, activity timeline, emitted events, telemetry bundle.
- Responsibilities:
  1. **Planning** – Invoke `decideAdvancedPlan` once per turn with preprocessed history (see Context Pipeline) and augment response with guardrails (confidence thresholds, fallback heuristics).
  2. **Tool Dispatch** – Route to `agenticRetrieveTool`, `webSearchTool`, or a combined branch. Planner fallbacks must surface rationale when overridden.
  3. **Synthesis** – Call `answerTool` with structured context generated by the context pipeline and attach source metadata.
  4. **Critique Loop** – Run `enhancedCritiqueDraft` (and future evaluators) with retry policies from `withRetry` (`backend/src/utils/resilience.ts:20`).
  5. **Telemetry** – Emit step-level events (start/finish, tokens in/out, cost estimates) appended to a structured trace.
- Refactor `handleEnhancedChat` and `handleChatStream` to delegate to the orchestrator. Streaming will read from orchestrator events rather than owning planning logic.
- Provide dependency injection for tools and telemetry sinks to make local testing easier (mock retrieval, stub critics).

### Context Pipeline
- **Sanitized History View** – Start from `sanitizeInput` output (`backend/src/middleware/sanitize.ts:6`). Apply a configurable turn limit (e.g., 12 most recent user/assistant messages) and strip redundant assistant echoes.
- **Rolling Summary** – Introduce `conversationSummarizer` module that compacts older turns once the token estimate exceeds threshold T. Persist summary entries in a scratchpad store (in-memory map initially, later external KV).
- **Salience Store** – Capture key facts, user preferences, and unresolved questions using prompt-based extraction after each assistant reply. Store as structured records with metadata (topic, lastSeen turn, decay score).
- **Selection/Compression** – Before calling retrieval or planner, assemble context by:
  1. Latest N raw turns.
  2. Most relevant summary snippets (semantic similarity against current user turn).
  3. Top-K salience notes filtered by freshness.
- **Token Budgeting** – Implement a `ContextBudget` helper that tracks estimated tokens using model-specific tokenizers and trims inputs to stay within configurable ceilings (e.g., 3k for planner, 6k for retrieval). Expose metrics in telemetry.

### Execution Flow
1. **Input Receipt** – `/chat` and `/chat/stream` both call `runSession` with the sanitized message list and a generated session id.
2. **Context Preparation** – Orchestrator builds context snapshot (raw turns + summaries + salience) via Context Pipeline.
3. **Planning** – `decideAdvancedPlan` executes on compacted history. If confidence < threshold, orchestrator may escalate to dual retrieval (knowledge + web) and annotate reason.
4. **Retrieval** – Execute primary tool path with `withRetry`. On failure, fall back to secondary (vector search, web search) while logging decision metadata.
5. **Synthesis & Critique** – `answerTool` consumes curated context; critic loop runs until acceptance or retry limit. Results appended to trace.
6. **Event Emission** – For streaming mode, orchestrator streams events as they occur (planning, retrieval, token deltas, critique) to the caller. Sync mode aggregates and returns final payload + trace.
7. **Telemetry Persist** – At completion, orchestrator writes trace to telemetry sink (initially in-memory with `/admin/telemetry`, later pluggable).

### Data & API Changes
- **Session Trace Schema** – Define new type in `shared/types.ts` (e.g., `SessionTrace`) capturing steps, messages, tool calls, token usage, errors.
- **Route Responses** – `/chat` responses include `traceId`, `plan`, `contextBudget`, and `criticSummary` inside `metadata`. SSE stream emits new events: `plan`, `context`, `tool`, `telemetry` for richer UX.
- **Configuration** – Add context budget limits, summary thresholds, and telemetry sinks to `config/app.ts` with sane defaults.

## Implementation Plan

### Phase 1 – Orchestrator Skeleton (1 sprint)
1. Create orchestrator module with interfaces and integrate into `/chat` while preserving current outputs.
2. Update `/chat/stream` to consume orchestrator events, falling back to aggregated output until full streaming support is ready.
3. Add unit tests for orchestrator decision flow using mocked tools.

### Phase 2 – Context Pipeline MVP (1–2 sprints)
1. Implement rolling summaries and salience store with in-memory persistence.
2. Introduce context budgeting helper and integrate into planner/retrieval calls.
3. Expand telemetry to record context components and token costs; expose via `/admin/telemetry`.

### Phase 3 – Tool Routing & Critique Enforcement (1 sprint)
1. Wire planner actions to actual tool invocations (enable Bing web search path, combined retrieval).
2. Ensure critic loop runs for both sync and streaming flows with consistent metadata.
3. Update frontend to display new events (planning status, context snapshots, critique summary).

### Phase 4 – Hardening (ongoing)
1. Add integration tests covering orchestration permutations (answer, retrieve, web search, fallback vector).
2. Stress-test context budgets with long conversations; refine summary heuristics.
3. Evaluate telemetry storage options (OpenTelemetry exporter, durable store).

## Open Questions
- What persistence layer do we adopt for summaries/salience (Redis, Azure Table) once in-memory proves insufficient?
- How do we redact sensitive user data inside stored traces without losing debugging fidelity?
- Should planner confidence thresholds be static or dynamically tuned based on evaluation feedback?

